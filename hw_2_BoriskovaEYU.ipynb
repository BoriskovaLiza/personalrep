{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whiAANkwhw-I"
   },
   "source": [
    "# Задание 1. MDP (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XccGClM6h3jh"
   },
   "source": [
    "Когда Инокетений Вальдемарович Носочков приходит на работу, то с вероятностью 50% он хочет пойти в бар, с 10% – покушац, с 20% – спать. Если он ничего не хочет, то он продолжает работать. Когда Кеша пьет в Баре, то в 10% посещений он возвращается на работу и с вероятностью 30% идет спать, но в остальное время продолжает пить. Когда он просыпается, то с вероятностью 40% идет покушац и с вероятностью 60% идет в бар пить дальше. Если вдруг г-н Носочков поел, то с вероятностью 80% он начинает работать, а если не срослось с работой, то он начинает хотеть спать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS-kwBSRh3mX"
   },
   "source": [
    "Определите вероятности, что наш герой прямо сейчас работает, пьет в баре, спит или ест, при условии, что если Инокетений чего-то хочет, то делает это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QIic4ugEhxYk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect Kesha's work-life balance formula:\n",
      "     working - 0.17, sleeping - 0.21, eating - 0.10, drinking - 0.52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#let's define Nosochkov's policy\n",
    "#working, sleeping, eating, drinking\n",
    "M = np.array([[0.2, 0.2, 0.1, 0.5],\n",
    "                [0, 0, 0.4, 0.6],\n",
    "                [0.8, 0.2, 0, 0],\n",
    "                [0.1, 0.3, 0, 0.6]])\n",
    "\n",
    "answer = np.ones((4)) * 0.25 @ np.linalg.matrix_power(M, 1000)\n",
    "print(\"Perfect Kesha's work-life balance formula:\\n \\\n",
    "    working - %.2f, sleeping - %.2f, eating - %.2f, drinking - %.2f\" % tuple(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMASlwFTie0R"
   },
   "source": [
    "# Задание 2. Tabular Q-learning  (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYSJPGRHpmz-"
   },
   "source": [
    "Условие игры: <br>\n",
    "Игра начинается с 1 очка. <br>Два игрока по очереди прибавляют очки от 1 до 9. <br>Побеждает тот, кто первым наберет 100 и более очков.\n",
    "\n",
    "Пример: <br>В игре 87 очков. <br>Первый прибавляет 7 -> получается 94. <br>Второй прибавляет 7, получается 101 и второй побеждает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "26xThksptHo8"
   },
   "outputs": [],
   "source": [
    "class Game:\n",
    "    \"\"\"\n",
    "    A class to represent the game\n",
    "    \"\"\"\n",
    "    points = 1\n",
    "    turn = 0 # player who made the previous move \n",
    "        \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes a new game\n",
    "        \"\"\"\n",
    "        self.points = 1\n",
    "        self.turn = 2\n",
    "        return\n",
    "\n",
    "    def reset_game(self):\n",
    "        \"\"\"\n",
    "        Resets the game to starting position\n",
    "        \"\"\"\n",
    "        self.points = 1\n",
    "        self.turn = 2\n",
    "        return\n",
    "    \n",
    "    def make_move(self, points):\n",
    "        \"\"\"\n",
    "        Player makes a move\n",
    "        \"\"\"\n",
    "        if points < 1 or points > 9:\n",
    "            print(\"Invalid points\")\n",
    "            return\n",
    "        if self.is_game_over()[0]:\n",
    "            return\n",
    "        self.points += points\n",
    "        self.turn = int(not(self.turn-1))+1\n",
    "        return\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        \"\"\"\n",
    "        Finds out weather the current game is over or not.\n",
    "        \"\"\"\n",
    "        if self.points >= 100:\n",
    "            return True, self.turn, 1 # Game ended, winner, reward\n",
    "        return False, self.turn, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "kkEpWG3xya7y"
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "class Random_player:\n",
    "    \"\"\"\n",
    "    Player class\n",
    "    \"\"\"\n",
    "    def move(self, points):\n",
    "        \"\"\"\n",
    "        Player makes a move.\n",
    "        \"\"\"\n",
    "        move = randrange(9) + 1\n",
    "        return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "5h-AyRuCwWdN"
   },
   "outputs": [],
   "source": [
    "game = Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Dsy7FoOywdXc"
   },
   "outputs": [],
   "source": [
    "rplayer = Random_player()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZN5f3lg4XCl",
    "outputId": "052982e6-9fc6-458c-896c-a68d97fe5688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random player victory ratio: 54 %\n"
     ]
    }
   ],
   "source": [
    "reward = 0\n",
    "for j in range(100):\n",
    "    game.reset_game()\n",
    "    while game.is_game_over()[0] == False:\n",
    "        game.make_move(rplayer.move(game.points))\n",
    "    if game.turn == 1:\n",
    "        reward += 1\n",
    "print(\"Random player victory ratio: %d\" % reward, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI6H52D5rsGa"
   },
   "source": [
    "## Победите 10/10 используя Tabular Q-learning.\n",
    "Вы играете за 1го игрока, второй игрок ходит рандомно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "nkpsBpDhhxfJ"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "n = 9\n",
    "Q = defaultdict(lambda: np.zeros(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(Q, state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(n)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "    \n",
    "def q_eval(Q, render=False):\n",
    "    \n",
    "    total_reward = []\n",
    "    \n",
    "    for game_round in range(10):\n",
    "        \n",
    "        game.reset_game()\n",
    "        done = game.is_game_over()[0]\n",
    "        \n",
    "        while not done:\n",
    "            action = int(np.argmax(Q[game.points]))\n",
    "            game.make_move(action + 1)\n",
    "            \n",
    "            game.make_move(rplayer.move(game.points))\n",
    "            done, turn, round_reward = game.is_game_over()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_reward.append(round_reward)        \n",
    "    return np.mean(total_reward)\n",
    "\n",
    "def q_learning(env, num_rounds, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "\n",
    "    Q = defaultdict(lambda: np.zeros(n))\n",
    "    round_lengths = np.zeros(num_rounds)\n",
    "    round_rewards = np.zeros(num_rounds)\n",
    "    \n",
    "    for game_round in range(num_rounds):\n",
    "        \n",
    "        game.reset_game()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state = game.points\n",
    "            action = int(eps_greedy(Q, state, epsilon-epsilon/num_rounds*game_round))\n",
    "            game.make_move(action + 1)\n",
    "        \n",
    "            round_lengths[game_round] += 1\n",
    "            game.make_move(player.move(game.points))\n",
    "            done, turn, reward = game.is_game_over()\n",
    "                    \n",
    "            round_rewards[game_round] += reward\n",
    "            next_state = game.points\n",
    "            \n",
    "            td_target = reward + discount_factor * np.max(Q[next_state])\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_error\n",
    "    \n",
    "    return Q, round_lengths, round_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q, round_lengths, round_rewards = q_learning(game, 1000)\n",
    "q_eval(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AV2oOocOiluq"
   },
   "source": [
    "# Задание 3. DQN (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdC2ifsfsLc3"
   },
   "source": [
    "## Победите 10/10 используя DQN.\n",
    "Подготовте алгоритм на основе Tabular Q-learning для 2го игрока.\n",
    "\n",
    "Вы играете за 1го игрока, победите 2го."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learning_Player:\n",
    "    \"\"\"\n",
    "    Player class\n",
    "    \"\"\"\n",
    "    def move(self, points):\n",
    "        \"\"\"\n",
    "        Player makes a move.\n",
    "        \"\"\"\n",
    "        global Q\n",
    "        action = int(np.argmax(Q[points]))\n",
    "        return action + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "qplayer = Q_learning_Player()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "bWdPYg3milur"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, states, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=1, stride=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=1, stride=1)\n",
    "        self.conv3 = nn.Conv1d(32, 32, kernel_size=1, stride=1)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = 1, stride = 1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        linear_input_size = conv2d_size_out(conv2d_size_out(conv2d_size_out(states)))\n",
    "        self.head = nn.Linear(32, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 1, 1)\n",
    "        x = x.type(torch.float32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.005\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "n_states = 108\n",
    "n_actions = 9\n",
    "\n",
    "policy_net = DQN(n_states, n_actions).to(device)\n",
    "target_net = DQN(n_states, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "round_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, flag = False):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_START - EPS_START / num_rounds * (steps_done + 1)\n",
    "    steps_done += 1\n",
    "    if flag:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "    \n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                        if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 1000\n",
    "\n",
    "for game_round in range(num_rounds):\n",
    "    steps_done = game_round\n",
    "    game.reset_game()\n",
    "    done = False\n",
    "    \n",
    "    for t in count():\n",
    "        state = np.array([[game.points]], dtype=np.int)\n",
    "        state = torch.from_numpy(state)\n",
    "\n",
    "        state = state.unsqueeze(0).to(device).type(torch.float32)\n",
    "        action = select_action(state)\n",
    "        game.make_move(action + 1)\n",
    "        game.make_move(qplayer.move(game.points))\n",
    "\n",
    "        done, turn, reward = game.is_game_over()\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if not done:\n",
    "            next_state = np.array([[game.points]], dtype=np.int)\n",
    "            next_state = torch.from_numpy(next_state)\n",
    "            next_state = state.unsqueeze(0).to(device).type(torch.float32)\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            round_durations.append(t + 1)\n",
    "            break\n",
    "    if game_round % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_eval(Q, render=False):\n",
    "    total_reward = []\n",
    "    \n",
    "    for game_round in range(10):\n",
    "        \n",
    "        game.reset_game()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state = np.array([[game.points]], dtype=np.int)\n",
    "            state = torch.from_numpy(state)\n",
    "            state = state.unsqueeze(0).to(device).type(torch.float32)\n",
    "            action = select_action(state, flag=True)\n",
    "            \n",
    "            game.make_move(action + 1)\n",
    "            done, turn, round_reward = game.is_game_over()\n",
    "            if done: break\n",
    "                \n",
    "            game.make_move(qplayer.move(game.points))\n",
    "            done, turn, round_reward\n",
    "            if done: break\n",
    "                \n",
    "        total_reward.append(round_reward)        \n",
    "    return np.mean(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn_eval(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8eTuu5WszL-"
   },
   "source": [
    "# Задание 4 (+2 балла при сделанном задании)\n",
    "Опишите выигрышную стратегию для первого игрока."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ir18OJZCs9c7"
   },
   "source": [
    "Выигрышной стратегией, думаю, будет прибавление с целью получаения 90 на ходе игрока 1 <br>\n",
    "То есть просто когда игра на ходе 1 игрока доходит до, к примеру, 87, он прибавляет именно 3, после этого 2 игрок прибавляя сколько угодно (от 1 до 9 в пределах правил) не получит больше 100, при это прибавление даже 1 позволит 1 игроку следующим шагом получить 100 <br>\n",
    "Игра вроде простая, так что обошлась без латеха :)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW_RL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
